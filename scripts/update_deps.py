import hashlib
import json
import os
import re
import shutil
import sys

import requests


def calculate_sha256(filepath):
    """Calculates the SHA256 hash of a file."""
    hasher = hashlib.sha256()
    with open(filepath, "rb") as f:
        while True:
            chunk = f.read(8192)  # Read in 8KB chunks
            if not chunk:
                break
            hasher.update(chunk)
    return hasher.hexdigest()


def download_file(url, download_path):
    """Downloads a file from a URL with progress."""
    print(f"  Downloading: {url}")
    try:
        with requests.get(url, stream=True, timeout=30) as r:
            r.raise_for_status()  # Raise HTTPError for bad responses
            total_size = int(r.headers.get("content-length", 0))
            downloaded_size = 0
            with open(download_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    if total_size > 0:
                        progress = int(50 * downloaded_size / total_size)
                        print(
                            f"\r  [{'=' * progress}{' ' * (50 - progress)}] {downloaded_size}/{total_size} bytes",
                            end="",
                        )
                print("\r" + " " * 80 + "\r", end="")  # Clear progress line
            print(f"  Downloaded to: {download_path}")
        return True
    except requests.exceptions.RequestException as e:
        print(f"\nError downloading {url}. Reason: {e}")
        # Clean up partially downloaded file
        if os.path.exists(download_path):
            os.remove(download_path)
        return False


def get_current_versions_from_cmake(cmake_filepath):
    """Reads all 'NAME_VERSION' variables from an existing versions.cmake file."""
    versions = {}
    if os.path.exists(cmake_filepath):
        with open(cmake_filepath, "r") as f:
            content = f.read()
            # Find all instances of set(SOME_NAME_VERSION "...")
            matches = re.findall(r"set\(([A-Z0-9_]+_VERSION) \"(.*?)\"\)", content)
            for match in matches:
                # Store as {"SOME_NAME_VERSION": "version_string"}
                versions[match[0]] = match[1]
    return versions


def generate_cmake_versions_file(versions_data, cmake_output_filepath):
    """Generates the versions.cmake file dynamically from the processed data."""
    with open(cmake_output_filepath, "w") as f:
        f.write("# This file is automatically generated by scripts/update_deps.py\n")
        f.write("# DO NOT EDIT THIS FILE MANUALLY!\n\n")

        for dep_name, dep_info in versions_data.items():
            if dep_name == "hashes":
                continue  # Skip the hashes section itself

            # --- Generic handling for simple git-tag based dependencies ---
            # These are identified by having a 'version' and 'url_template' but not a 'components' dictionary.
            if isinstance(dep_info, dict) and "version" in dep_info and dep_info.get("base_maven_url") is None:
                f.write(f"# --- {dep_name.replace('_', ' ').title()} ---\n")
                f.write(f"set({dep_name.upper()}_VERSION \"{dep_info['version']}\")\n\n")

            # --- Specific, complex handling for WPILib ---
            if dep_name == "wpi_libs":
                f.write("# --- WPILib ---\n")
                f.write(f"set(WPILIB_VERSION \"{dep_info['version']}\")\n")
                f.write(
                    f"set(WPILIB_MAVEN_BASE_URL \"{dep_info['base_maven_url']}\")\n\n"
                )

                architectures = ["x86_64", "arm64"]
                build_types = ["release", "debug"]

                for comp_name, comp_info in dep_info["components"].items():
                    f.write(f"# {comp_name}\n")
                    for arch in architectures:
                        for build_type in build_types:
                            fetch_name = comp_info["fetch_name_template"].format(
                                arch=arch, type=build_type
                            )
                            if fetch_name in versions_data["hashes"]:
                                url_var = f"{comp_name.upper()}_{arch.upper()}_BIN_{build_type.upper()}_URL"
                                hash_var = f"{comp_name.upper()}_{arch.upper()}_BIN_{build_type.upper()}_HASH"

                                f.write(
                                    f"set({url_var} \"{versions_data['hashes'][fetch_name]['url']}\")\n"
                                )
                                f.write(
                                    f"set({hash_var} \"SHA256={versions_data['hashes'][fetch_name]['hash']}\")\n"
                                )

                    headers_fetch_name = comp_info["headers_fetch_name"]
                    if headers_fetch_name in versions_data["hashes"]:
                        headers_url_var = f"{comp_name.upper()}_HEADERS_URL"
                        headers_hash_var = f"{comp_name.upper()}_HEADERS_HASH"
                        f.write(
                            f"set({headers_url_var} \"{versions_data['hashes'][headers_fetch_name]['url']}\")\n"
                        )
                        f.write(
                            f"set({headers_hash_var} \"SHA256={versions_data['hashes'][headers_fetch_name]['hash']}\")\n"
                        )
                    f.write("\n")
    print(f"Generated {cmake_output_filepath}")

def check_if_update_needed(desired_versions_data, cmake_versions_file):
    """Dynamically checks if any dependency version has changed."""
    if not os.path.exists(cmake_versions_file):
        print("--- versions.cmake not found. Update required. ---")
        return True

    current_versions = get_current_versions_from_cmake(cmake_versions_file)
    needs_update = False

    print("--- Comparing versions... ---")
    for dep_name, dep_info in desired_versions_data.items():
        if isinstance(dep_info, dict) and "version" in dep_info:
            cmake_var_name = f"{dep_name.upper()}_VERSION"
            desired_version = dep_info["version"]
            current_version = current_versions.get(cmake_var_name)

            print(f"  {dep_name}: Desired='{desired_version}', Current='{current_version}'")

            if current_version != desired_version:
                print(f"    -> Mismatch for {dep_name}. Update required.")
                needs_update = True

    if not needs_update:
        print("--- All dependency versions match. Skipping downloads and hash generation. ---")

    return needs_update


def main():
    script_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(script_dir, os.pardir))
    versions_json_file = os.path.join(project_root, "versions.json")
    cmake_versions_file = os.path.join(project_root, "third_party", "versions.cmake")
    temp_download_dir = os.path.join(project_root, ".tmp_deps_downloads")

    os.makedirs(temp_download_dir, exist_ok=True)

    try:
        with open(versions_json_file, "r") as f:
            desired_versions_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: {versions_json_file} not found. Please create it.")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: {versions_json_file} is not valid JSON. Please fix it.")
        sys.exit(1)

    if not check_if_update_needed(desired_versions_data, cmake_versions_file):
        sys.exit(0)

    print("\n--- Starting Downloads and Hash Generation for required components. ---")

    # --- Dynamically process dependencies that need downloading ---
    for dep_name, dep_info in desired_versions_data.items():
        # Only process wpi_libs, as others are handled by FetchContent in CMake
        if dep_name == "wpi_libs":
            print("\nProcessing WPILib Components...")
            wpi_version = dep_info["version"]
            wpi_base_maven_url = dep_info["base_maven_url"]
            architectures = {"x86_64": "linuxx86-64", "arm64": "linuxarm64"}
            build_types = {"release": "", "debug": "debug"}

            for comp_name, comp_info in dep_info["components"].items():
                print(f"\n  Processing WPILib {comp_name} v{wpi_version}")

                # Handle binary downloads
                for arch_key, arch_classifier in architectures.items():
                    for build_type_key, build_type_classifier in build_types.items():
                        classifier_suffix = f"{arch_classifier}{build_type_classifier}"
                        binary_url = comp_info["binary_url_template"].format(
                            base_maven_url=wpi_base_maven_url,
                            group_id_path=comp_info["group_id_path"],
                            artifact_id=comp_info["artifact_id"],
                            version=wpi_version,
                            classifier=classifier_suffix,
                        )
                        filename = f"{comp_info['artifact_id']}-{wpi_version}-{classifier_suffix}.zip"
                        download_path = os.path.join(temp_download_dir, filename)
                        fetch_name = comp_info["fetch_name_template"].format(
                            arch=arch_key, type=build_type_key
                        )

                        if download_file(binary_url, download_path):
                            binary_hash = calculate_sha256(download_path)
                            desired_versions_data["hashes"][fetch_name] = {
                                "url": binary_url, "hash": binary_hash
                            }
                            print(f"    {fetch_name} hash: {binary_hash}")
                        else:
                            sys.exit(1) # Exit if download fails

                # Handle header downloads
                headers_url = comp_info["headers_url_template"].format(
                    base_maven_url=wpi_base_maven_url,
                    group_id_path=comp_info["group_id_path"],
                    artifact_id=comp_info["artifact_id"],
                    version=wpi_version,
                )
                headers_filename = f"{comp_info['artifact_id']}-{wpi_version}-headers.zip"
                headers_download_path = os.path.join(temp_download_dir, headers_filename)
                headers_fetch_name = comp_info["headers_fetch_name"]

                if download_file(headers_url, headers_download_path):
                    headers_hash = calculate_sha256(headers_download_path)
                    desired_versions_data["hashes"][headers_fetch_name] = {
                        "url": headers_url, "hash": headers_hash
                    }
                    print(f"    {headers_fetch_name} hash: {headers_hash}")
                else:
                    sys.exit(1) # Exit if download fails

    # --- Final Steps ---
    print("\n--- Finalizing Update ---")

    # Save updated versions.json (contains all calculated hashes)
    with open(versions_json_file, "w") as f:
        json.dump(desired_versions_data, f, indent=2)
        f.write("\n") # Add trailing newline for POSIX compatibility
    print(f"Updated {versions_json_file} with new hashes.")

    # Generate versions.cmake from the updated data
    generate_cmake_versions_file(desired_versions_data, cmake_versions_file)

    shutil.rmtree(temp_download_dir)
    print(f"Cleaned up temporary directory: {temp_download_dir}")

    print("\nDependency update process completed.")
    print("Remember to 'git add' and 'git commit' the updated versions.json and versions.cmake files.")

if __name__ == "__main__":
    main()
